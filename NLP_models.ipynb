{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DesQBueszHiv",
        "5kLvQbDa0QUp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> The goal of this notebook is to explore 3 different NLP techniques in order to better predict the tone of our news data. To do so, we have selected models from 'simple' level to state of the art tehcnique:\n",
        "- FinBERT\n",
        "- Vader\n",
        "- TF-IDF"
      ],
      "metadata": {
        "id": "H-fWu09H1Ix6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Loading data"
      ],
      "metadata": {
        "id": "eMQPiRHBy2ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
        "from NowcastingEco import NowcastingEco\n",
        "from tqdm import tqdm\n",
        "\n",
        "finance_news = pd.read_excel('new_annotated_articles2.xlsx')\n",
        "finance_news.head()"
      ],
      "metadata": {
        "id": "UqiBjqOpyynB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'Headlines' contains only strings and contains no empty elements\n",
        "headline_strings_only = finance_news['headline'].dtype == object\n",
        "print(\"Headlines contains only strings:\", headline_strings_only)\n",
        "print(\"Nb of empty headlines:\", finance_news['headline'].isnull().sum())"
      ],
      "metadata": {
        "id": "UJAqJyYky8UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FinBERT"
      ],
      "metadata": {
        "id": "hpI3R6opywuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7zqv1YsyrRx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_excel('new_annotated_articles2.xlsx')\n",
        "# Replace -1 values with 2 in 'annotated_tone' to match expected outputs of finBERT\n",
        "df['annotated_tone'] = df['annotated_tone'].replace(-1, 2)\n",
        "# Split data into training and testing sets\n",
        "train_text, test_text, train_labels, test_labels = train_test_split(df['headline'], df['annotated_tone'], test_size=0.2)\n",
        "\n",
        "# Initialize the finBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# If there's a GPU available\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# move the model to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Create a PyTorch Dataset\n",
        "class FinSentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = FinSentimentDataset(train_encodings, train_labels.tolist())\n",
        "test_dataset = FinSentimentDataset(test_encodings, test_labels.tolist())\n",
        "train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=40, shuffle=True)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(5):  \n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        print(input_ids)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        print(attention_mask)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Batch Loss: {loss.item()}')\n",
        "    print(f'Average Loss after Epoch {epoch+1}: {total_loss/total_batches}')\n",
        "\n",
        "model.save_pretrained(\"/content/finBERT_pretrained_v1\")"
      ],
      "metadata": {
        "id": "HwplC3ETy-TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "    print(outputs.logits)\n",
        "    print(predictions)\n",
        "    correct += (predictions == labels).sum().item()\n",
        "    total += labels.numel()\n",
        "\n",
        "print(f'Accuracy: {correct/total}')"
      ],
      "metadata": {
        "id": "H44PbE5pzAvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Past results:**\n",
        "\n",
        "> Epochs: 3 - lr: 1e-5 - Batch size: 16 - **Accuracy: 0.76 / Loss:**\n",
        "\n",
        "> Epochs: 3 - lr: 1e-4 - Batch size: 32 - **Accuracy: 0.79 / Loss: 0.25106**\n",
        "\n",
        "> Epochs: 5 - lr: 5e-5 - Batch size: 32 - **Accuracy: 0.795 / Loss: 0.14585**\n",
        "\n",
        "> Epochs: 5 - lr: 1e-4 - Batch size: 64 - **Accuracy: 0.77 / Loss: 0.13444**\n",
        "\n",
        "> Epochs: 5 - lr: 1e-4 - Batch size: 32 - **Accuracy:0.77 / Loss: 0.1806** \n",
        "\n",
        "> Epochs: 5 - lr: 5e-5 - Batch size: 32 - **Accuracy:0.73 / Loss: 0.1089**\n",
        "\n",
        "> Epochs: 5 - lr: 5e-5 - Batch size 40 - **Accuracy: 0.82 / Loss: 0.15391**"
      ],
      "metadata": {
        "id": "mC7-iogSzDgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply FinBERT on our news data to predict a new tone:"
      ],
      "metadata": {
        "id": "TByuBobHDbqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with NowcastingEco() from news data\n",
        "df = pd.read_csv('/Users/amaury/Documents/!DSBA/CRP/headlines_english_arabic_countries.csv')\n",
        "crash_test = NowcastingEco(df)\n",
        "crash_test.clean_data() # Egypt\n",
        "\n",
        "#For Egypt\n",
        "df_egypt = crash_test.df"
      ],
      "metadata": {
        "id": "7E2ngmoyEFbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/finBERT_pretrained_v1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\", force_download=True, resume_download=False)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"
      ],
      "metadata": {
        "id": "VKtnmiXJDgMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tone(text):\n",
        "    #print(text)\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(device)\n",
        "    #print(inputs)\n",
        "    outputs = model(**inputs)\n",
        "    predicted = np.argmax(outputs.logits.detach().cpu().numpy())\n",
        "    #print(predicted)\n",
        "    return predicted\n",
        "\n",
        "\n",
        "# Apply the prediction function to the adjust_headline column with progress bar\n",
        "tqdm.pandas()\n",
        "df_egypt['tone_prediction'] = df_egypt['title'].progress_apply(predict_tone)"
      ],
      "metadata": {
        "id": "2ugY8uepDtS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export data with the new tones\n",
        "#df['tone_prediction'] = df['tone_prediction'].replace(1, \"positive\")\n",
        "#df['tone_prediction'] = df['tone_prediction'].replace(2, \"negative\")\n",
        "#df_egypt.to_csv('/content/drive/MyDrive/egypt_tone_pred_v2.csv', index=True)"
      ],
      "metadata": {
        "id": "_MXe45WgD6cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vader"
      ],
      "metadata": {
        "id": "DesQBueszHiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary packages and the excel sheet with annotated articles\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "test_set = pd.read_excel('/Users/jeanlahellec/Downloads/new_annotated_articles2.xlsx')\n",
        "test_set.head()"
      ],
      "metadata": {
        "id": "UdC9uGqJzJaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying vader to the headlines\n",
        "test_set['scores'] = test_set['headline'].apply(lambda title: sid.polarity_scores(title) if pd.notnull(title) else None)\n",
        "test_set= test_set.dropna(subset=['scores'])\n",
        "test_set.shape"
      ],
      "metadata": {
        "id": "OzbKjLYCzNHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting only the compound score (regularized compounded score of neutral, positive and negative scores)\n",
        "test_set['compound']  = test_set['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "test_set['comp_score'] = test_set['compound'].apply(lambda c: 1 if c >=0 else -1)\n",
        "test_set.head()"
      ],
      "metadata": {
        "id": "ueKOBYQpzRDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'actual' column represents the ground truth labels\n",
        "actual_labels = test_set['annotated_tone'].values\n",
        "\n",
        "# Assuming 'predicted' column represents the predicted labels\n",
        "predicted_labels = test_set['comp_score'].values\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "accuracy_classes=classification_report(actual_labels, predicted_labels)\n",
        "# Calculate recall\n",
        "recall = recall_score(actual_labels, predicted_labels)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(actual_labels, predicted_labels)\n",
        "\n",
        "print(accuracy_classes)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "mK_MDUL8zSaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "5kLvQbDa0QUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'Headlines' contains only strings and contains no empty elements\n",
        "headline_strings_only = finance_news['headline'].dtype == object\n",
        "print(\"Headlines contains only strings:\", headline_strings_only)\n",
        "print(\"Nb of empty headlines:\", finance_news['headline'].isnull().sum())\n",
        "\n",
        "finance_news[finance_news['headline'].isnull()]"
      ],
      "metadata": {
        "id": "3BXmG28z0Rfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grams"
      ],
      "metadata": {
        "id": "WyKQUthE0s0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing\n",
        "finance_news = finance_news[['headline', 'annotated_tone']].dropna()\n",
        "\n",
        "# Step 2: Splitting the data\n",
        "X = finance_news['headline']\n",
        "y = finance_news['annotated_tone']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Vectorizing the data with unigrams, bigrams, and trigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 4: Training different classifiers\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    SVC(),\n",
        "    MultinomialNB(),\n",
        "    RandomForestClassifier(),\n",
        "]\n",
        "\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Step 5: Predicting and evaluating\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{classifier.__class__.__name__} Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "5uyWzFWj0gLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF + Word2Vec"
      ],
      "metadata": {
        "id": "GHtIL1GE0uJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing\n",
        "finance_news = finance_news[['headline', 'annotated_tone']].dropna()\n",
        "X = finance_news['headline']\n",
        "y = finance_news['annotated_tone']\n",
        "\n",
        "# Step 2: TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Word Embedding Generation\n",
        "embedding_size = 100  # Specify the desired size of word embeddings\n",
        "window_size = 2  # Specify the context window size for Word2Vec\n",
        "min_word_count = 1  # Specify the minimum word count threshold for Word2Vec\n",
        "word_embeddings = Word2Vec(sentences=[sentence.split() for sentence in X], vector_size=embedding_size, window=window_size, min_count=min_word_count)\n",
        "\n",
        "# Step 4: Combine TF-IDF and Word Embeddings\n",
        "combined_embeddings = []\n",
        "for headline in X:\n",
        "    word_embedding_weights = []\n",
        "    for word in headline.split():\n",
        "        if word in word_embeddings.wv and word in vectorizer.vocabulary_:\n",
        "            tfidf_weight = vectorizer.idf_[vectorizer.vocabulary_[word]]\n",
        "            word_embedding_weights.append(tfidf_weight * word_embeddings.wv[word])\n",
        "    if word_embedding_weights:\n",
        "        headline_embedding = np.mean(word_embedding_weights, axis=0)\n",
        "    else:\n",
        "        headline_embedding = np.zeros(embedding_size)\n",
        "    combined_embeddings.append(headline_embedding)\n",
        "\n",
        "# Step 5: Train and Evaluate Different Classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    'Random Forest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "for clf_name, classifier in classifiers.items():\n",
        "    print(f\"Classifier: {clf_name}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, y, test_size=0.2, random_state=42)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    \n",
        "    # Predict and Output the Tone for Each News\n",
        "    predicted_tone = classifier.predict(combined_embeddings)\n",
        "    finance_news['predicted_tone'] = predicted_tone\n",
        "    print(finance_news[['headline', 'annotated_tone', 'predicted_tone']])\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "-SA2LPMd0w9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
